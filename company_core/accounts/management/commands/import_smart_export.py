import json
import re
from pathlib import Path
from tempfile import TemporaryDirectory
from zipfile import ZipFile

from django.apps import apps
from django.contrib.auth import get_user_model
from django.core.management import BaseCommand, CommandError, call_command
from django.core.exceptions import FieldDoesNotExist
from django.db import IntegrityError
from django.db import transaction

User = get_user_model()


def _parse_skip_list(value: str | None) -> set[str]:
    if not value:
        return set()
    return {item.strip() for item in value.split(",") if item.strip()}


def _make_unique_username(base: str, suffix: str) -> str:
    base = (base or "").strip() or "imported_user"
    # Keep usernames reasonably short to avoid max_length surprises.
    base = base[:140]
    candidate = base
    i = 0
    while User.objects.filter(username__iexact=candidate).exists():
        i += 1
        candidate = f"{base}_{suffix}{i}"
        candidate = candidate[:150]
    return candidate


def _model_key(model_cls) -> tuple[str, str]:
    return (model_cls._meta.app_label, model_cls._meta.model_name)


def _label_lower(model_cls) -> str:
    return f"{model_cls._meta.app_label}.{model_cls._meta.model_name}"


def _is_int_pk(value) -> bool:
    # Django fixture PK/FK values are typically ints; natural keys are lists/strings.
    return isinstance(value, int) and value >= 0


def _split_unit_vin_combo(value: str | None) -> tuple[str | None, str | None]:
    if not isinstance(value, str):
        return None, None
    cleaned = re.sub(r"\s+", " ", value).strip()
    if not cleaned:
        return None, None
    pattern = re.search(
        r"\bunit\s*(?:#|no\.?|number)?\s*[:\-]?\s*([A-Za-z0-9\-\s]+?)\b.*?\bvin\s*(?:#|no\.?|number)?\s*[:\-]?\s*([A-Za-z0-9\-\s]+?)\b",
        cleaned,
        flags=re.IGNORECASE,
    )
    if pattern:
        return pattern.group(1).strip(), pattern.group(2).strip()
    for sep in ("/", "|", ";", ","):
        if sep in cleaned:
            parts = [part.strip() for part in cleaned.split(sep) if part.strip()]
            if len(parts) >= 2:
                return parts[0], parts[1]
    return cleaned, None


def _truncate_string(value: str, max_length: int | None) -> tuple[str, bool]:
    if not max_length or len(value) <= max_length:
        return value, False
    return value[:max_length], True


def _normalize_unit_vin_fields(fields: dict, unit_field_name: str | None, vin_field_name: str | None) -> None:
    if not unit_field_name:
        return
    raw_unit = fields.get(unit_field_name)
    if not isinstance(raw_unit, str):
        return
    parsed_unit, parsed_vin = _split_unit_vin_combo(raw_unit)
    if parsed_unit:
        fields[unit_field_name] = parsed_unit
    else:
        fields[unit_field_name] = raw_unit
    if parsed_vin and vin_field_name and not fields.get(vin_field_name):
        fields[vin_field_name] = parsed_vin


class Command(BaseCommand):
    help = (
        "Import the ZIP prepared by smart-invoices (accounts export) into expresslube."
    )

    def add_arguments(self, parser):
        parser.add_argument(
            "zip_path",
            type=str,
            help="Path to the ZIP file generated by prepare_express_import.",
        )
        parser.add_argument(
            "--skip-models",
            type=str,
            help="Comma-separated list of JSON filenames to skip during import.",
        )

    @transaction.atomic
    def handle(self, *args, **options):
        def log(message: str) -> None:
            self.stdout.write(message)
            try:
                self.stdout.flush()
            except Exception:
                pass

        skip_files = _parse_skip_list(options.get("skip_models"))
        zip_path = Path(options["zip_path"])
        log(f"Starting import from: {zip_path}")
        if skip_files:
            log(f"Skip list: {', '.join(sorted(skip_files))}")

        if not zip_path.exists():
            raise CommandError(f"ZIP file not found: {zip_path}")

        with TemporaryDirectory(prefix="express_import_") as temp_dir:
            temp_dir_path = Path(temp_dir)
            with ZipFile(zip_path, "r") as zipf:
                zipf.extractall(temp_dir_path)
            log(f"Extracted ZIP into temp dir: {temp_dir_path}")

            manifest_path = temp_dir_path / "manifest.json"
            ordered_files = []
            exported_user_map = {}
            local_user_map: dict[int, int] = {}
            created_users: dict[int, str] = {}
            manifest_models = []
            user_label_lower = _label_lower(User)
            import_tag = zip_path.stem.replace(".", "_").replace("-", "_")

            if manifest_path.exists():
                log("Found manifest.json; using its model order.")
                with open(manifest_path, "r", encoding="utf-8") as mf:
                    manifest = json.load(mf)
                exported_user_map = manifest.get("user_map", {})
                for entry in manifest.get("models", []):
                    manifest_models.append(entry.get("model"))
                    candidate = temp_dir_path / entry.get("file", "")
                    if candidate.exists():
                        ordered_files.append(candidate)
            else:
                log("No manifest.json found; scanning all JSON fixtures.")
                ordered_files = sorted(
                    f for f in temp_dir_path.glob("*.json") if f.name != "manifest.json"
                )

            if not ordered_files:
                raise CommandError("No JSON fixtures found in the provided ZIP.")

            truncation_warnings: set[tuple[str, str]] = set()

            # Reorder fixtures to satisfy common FK dependencies (products/services before income records, etc.)
            priority = {
                # Business/User-scoped setup
                "Profile.json": 1,
                "ServiceJobName.json": 2,
                # Catalog / master data
                "Category.json": 5,
                "Supplier.json": 9,
                "Product.json": 10,
                "Service.json": 10,
                "ServiceDescription.json": 11,
                "InventoryLocation.json": 15,
                "InventoryTransaction.json": 16,
                "Customer.json": 20,
                "Driver.json": 20,
                "FleetVehicle.json": 22,
                "Vehicle.json": 22,
                "GroupedEstimate.json": 40,
                "EstimateRecord.json": 41,
                "WorkOrder.json": 120,
                "WorkOrderRecord.json": 121,
                "GroupedInvoice.json": 50,
                "PendingInvoice.json": 51,
                "PaidInvoice.json": 52,
                "InvoiceDetail.json": 70,
                "IncomeRecord.json": 80,
                "IncomeRecord2.json": 80,
                "JobHistory.json": 200,
            }
            ordered_files = sorted(ordered_files, key=lambda p: (priority.get(p.name, 999), p.name))
            log(f"Queued {len(ordered_files)} fixture file(s) for import.")

            # IMPORTANT: This importer is append-only.
            # - Never delete existing rows.
            # - Never merge into an existing business/user.
            # - Always create NEW user account(s) and remap all PK/FK/M2M references.

            # Step 1: create NEW users and build exported_user_id -> local_user_id mapping
            # Prefer manifest user_map (exported_id -> username). If missing, fall back to scanning fixtures
            # for auth user entries and creating new users from them.
            if exported_user_map:
                log(f"Creating {len(exported_user_map)} user(s) from manifest map.")
                with transaction.atomic():
                    for exported_id_str, username in exported_user_map.items():
                        try:
                            exported_id = int(exported_id_str)
                        except (TypeError, ValueError):
                            continue
                        new_username = _make_unique_username(username, suffix=import_tag)
                        user_obj = User(username=new_username)
                        # Avoid importing passwords; set unusable and let admins reset if needed.
                        try:
                            user_obj.set_unusable_password()
                        except Exception:
                            pass
                        user_obj.save()
                        local_user_map[exported_id] = user_obj.pk
                        created_users[exported_id] = user_obj.username
                        log(f"Created user {user_obj.username} for exported id {exported_id}")
            else:
                # Scan fixtures for user objects (if present) and create new users.
                # This keeps the behavior safe even without a manifest.
                candidate_user_entries = []
                for fp in ordered_files:
                    if fp.name in skip_files:
                        continue
                    log(f"Scanning {fp.name} for user entries.")
                    try:
                        data = json.loads(fp.read_text(encoding="utf-8"))
                    except Exception:
                        continue
                    for obj in data if isinstance(data, list) else []:
                        if obj.get("model") == user_label_lower and _is_int_pk(obj.get("pk")):
                            candidate_user_entries.append(obj)
                if candidate_user_entries:
                    with transaction.atomic():
                        for obj in candidate_user_entries:
                            exported_id = obj["pk"]
                            fields = obj.get("fields", {}) or {}
                            username = fields.get("username") or f"user_{exported_id}"
                            new_username = _make_unique_username(username, suffix=import_tag)
                            user_obj = User(username=new_username)
                            email = fields.get("email")
                            if isinstance(email, str) and email.strip():
                                user_obj.email = email.strip()
                            try:
                                user_obj.set_unusable_password()
                            except Exception:
                                pass
                            user_obj.save()
                            local_user_map[exported_id] = user_obj.pk
                            created_users[exported_id] = user_obj.username
                            log(f"Created user {user_obj.username} for exported id {exported_id}")

            if created_users:
                preview = ", ".join([f"{eid}â†’{uname}" for eid, uname in sorted(created_users.items())])
                self.stdout.write(self.style.SUCCESS(f"Created new user account(s): {preview}"))

            # Temporarily disable IncomeRecord2 signals that auto-create inventory or job history
            try:
                log("Disabling IncomeRecord2-related signals during import.")
                income_record_model = apps.get_model("accounts", "IncomeRecord2")
                account_signals = __import__("accounts.signals", fromlist=["dummy"])
                from django.db.models.signals import post_save, post_delete

                post_save.disconnect(receiver=account_signals.sync_job_history, sender=income_record_model)
                post_save.disconnect(receiver=account_signals.ensure_inventory_for_line, sender=income_record_model)
                post_delete.disconnect(receiver=account_signals.delete_job_history, sender=income_record_model)
                grouped_invoice_model = apps.get_model("accounts", "GroupedInvoice")
                post_save.disconnect(receiver=account_signals.log_grouped_invoice_activity, sender=grouped_invoice_model)
            except Exception:
                pass

            # Step 2: load all non-user objects and import them with PK/FK remapping.
            # We intentionally do NOT use loaddata because it would reuse PKs and can overwrite/attach to existing rows.
            all_entries: list[dict] = []
            for file_path in ordered_files:
                if file_path.name in skip_files:
                    self.stdout.write(self.style.WARNING(f"Skipping {file_path.name} per request."))
                    continue
                log(f"Loading {file_path.name}")
                try:
                    data = json.loads(file_path.read_text(encoding="utf-8"))
                except Exception as exc:
                    raise CommandError(f"Failed reading {file_path.name}: {exc}") from exc
                if not isinstance(data, list):
                    continue
                log(f"Loaded {len(data)} records from {file_path.name}")
                for obj in data:
                    if not isinstance(obj, dict):
                        continue
                    model_label = obj.get("model") or ""
                    if model_label == user_label_lower:
                        # Users are created fresh above; never import/merge user rows.
                        continue
                    all_entries.append(obj)
            log(f"Prepared {len(all_entries)} object(s) for import.")

            # Precompute which PKs exist in the import per model (to decide whether an FK should be remapped).
            imported_pks: dict[tuple[str, str], set[int]] = {}
            for obj in all_entries:
                model_label = obj.get("model") or ""
                if "." not in model_label:
                    continue
                app_label, model_name = model_label.split(".", 1)
                pk = obj.get("pk")
                if _is_int_pk(pk):
                    imported_pks.setdefault((app_label, model_name), set()).add(pk)

            pk_map: dict[tuple[str, str], dict[int, int]] = {}
            m2m_queue: list[tuple[object, dict]] = []  # (instance, {field_name: [old_ids...]})
            last_errors: dict[str, str] = {}

            # Multi-pass import to satisfy FK dependencies even if ordering isn't perfect.
            pending = list(all_entries)
            max_passes = 25
            from django.utils import timezone

            for pass_idx in range(1, max_passes + 1):
                if not pending:
                    break
                log(f"Import pass {pass_idx} starting with {len(pending)} pending object(s).")
                created_this_pass = 0
                next_pending = []

                for obj in pending:
                    model_label = obj.get("model") or ""
                    if "." not in model_label:
                        continue
                    app_label, model_name = model_label.split(".", 1)
                    try:
                        model_cls = apps.get_model(app_label, model_name)
                    except Exception:
                        continue

                    old_pk = obj.get("pk")
                    fields = dict(obj.get("fields", {}) or {})

                    unit_field_name = None
                    vin_field_name = None
                    for field_obj in model_cls._meta.fields:
                        if field_obj.name in ("unit_no", "unit_number") and not unit_field_name:
                            unit_field_name = field_obj.name
                        if field_obj.name == "vin_no":
                            vin_field_name = field_obj.name

                    # Drop fields not present on the target model.
                    for key in list(fields.keys()):
                        try:
                            model_cls._meta.get_field(key)
                        except FieldDoesNotExist:
                            fields.pop(key, None)

                    # Fill required timestamp defaults when missing.
                    for f in model_cls._meta.fields:
                        if f.name in fields or f.auto_created:
                            continue
                        if getattr(f, "auto_now_add", False) or getattr(f, "auto_now", False):
                            fields[f.name] = timezone.now().isoformat()
                        elif f.has_default():
                            try:
                                val = f.get_default()
                                try:
                                    import datetime
                                    from decimal import Decimal
                                    if isinstance(val, (datetime.datetime, datetime.date)):
                                        val = val.isoformat()
                                    if isinstance(val, Decimal):
                                        val = str(val)
                                except Exception:
                                    pass
                                fields[f.name] = val
                            except Exception:
                                pass

                    _normalize_unit_vin_fields(fields, unit_field_name, vin_field_name)

                    for field_obj in model_cls._meta.fields:
                        max_length = getattr(field_obj, "max_length", None)
                        raw_value = fields.get(field_obj.name)
                        if isinstance(raw_value, str) and max_length:
                            truncated_value, truncated = _truncate_string(raw_value, max_length)
                            if truncated and (model_label, field_obj.name) not in truncation_warnings:
                                truncation_warnings.add((model_label, field_obj.name))
                                self.stdout.write(self.style.WARNING(
                                    f"Truncated {model_label}.{field_obj.name} to {max_length} characters."
                                ))
                            fields[field_obj.name] = truncated_value

                    # Extract M2M values to apply after save.
                    m2m_values: dict[str, list] = {}
                    for m2m in model_cls._meta.many_to_many:
                        if m2m.name in fields and isinstance(fields[m2m.name], list):
                            m2m_values[m2m.name] = fields.pop(m2m.name)

                    # Remap FK/O2O IDs.
                    unresolved = False
                    for field in model_cls._meta.fields:
                        if not getattr(field, "is_relation", False) or not getattr(field, "remote_field", None):
                            continue
                        if not field.concrete:
                            continue
                        fname = field.name
                        if fname not in fields:
                            continue
                        raw_val = fields.get(fname)

                        # Natural keys (lists/strings) aren't remapped here.
                        if not _is_int_pk(raw_val):
                            continue

                        remote_model = field.remote_field.model
                        attname = getattr(field, "attname", f"{fname}_id")
                        # Handle auth/user FKs: ALWAYS remap to newly created users.
                        if remote_model == User:
                            if raw_val in local_user_map:
                                fields.pop(fname, None)
                                fields[attname] = local_user_map[raw_val]
                            else:
                                fields.pop(fname, None)
                                fields[attname] = None
                            continue

                        r_key = _model_key(remote_model)
                        # If the referenced PK exists in the import, require mapping before we can save.
                        if raw_val in imported_pks.get(r_key, set()):
                            mapped = pk_map.get(r_key, {}).get(raw_val)
                            if mapped is None:
                                unresolved = True
                                break
                            fields.pop(fname, None)
                            fields[attname] = mapped
                        else:
                            # This FK points outside the imported dataset (likely shared lookup/config rows).
                            # Keep it as-is so required FKs don't get nulled and fail to save.
                            fields.pop(fname, None)
                            fields[attname] = raw_val

                    if unresolved:
                        next_pending.append(obj)
                        continue

                    # Ensure Service rows always have a job_name (per-user unique)
                    if model_cls._meta.model_name == "service":
                        # FK ints are stored in *_id keys by this importer.
                        if not fields.get("job_name_id") and fields.get("user_id"):
                            try:
                                job_model = apps.get_model("accounts", "ServiceJobName")
                                job_obj, _ = job_model.objects.get_or_create(
                                    user_id=fields["user_id"],
                                    name="General",
                                    defaults={"is_active": True},
                                )
                                fields["job_name_id"] = job_obj.pk
                            except Exception:
                                pass
                    # If JobHistory is missing total_job_cost (computed in save()), compute it now
                    # because we import using save_base(raw=True) to avoid side effects.
                    if model_cls._meta.model_name == "jobhistory":
                        if "total_job_cost" not in fields:
                            try:
                                from decimal import Decimal
                                service_cost = fields.get("service_cost") or "0.00"
                                tax_amount = fields.get("tax_amount") or "0.00"
                                fields["total_job_cost"] = str(Decimal(str(service_cost)) + Decimal(str(tax_amount)))
                            except Exception:
                                pass

                    # Create the instance with a new PK (never reuse old PK).
                    try:
                        with transaction.atomic(savepoint=True):
                            instance = model_cls(**fields)
                            # Save in "raw" mode like Django loaddata to avoid custom save() side effects
                            # (invoice creation, inventory mutation, etc.). This is critical to avoid
                            # touching existing businesses.
                            instance.save_base(raw=True, force_insert=True)
                    except IntegrityError as exc:
                        # Common case: Profile rows already exist because a signal auto-created them
                        # when we created the new user. In that case, update the existing row for
                        # the newly created user, and map the old PK to it.
                        if model_cls._meta.model_name == "profile" and _is_int_pk(fields.get("user_id")):
                            try:
                                existing = model_cls.objects.filter(user_id=fields["user_id"]).first()
                                if existing:
                                    update_fields = {k: v for k, v in fields.items() if k != "user_id"}
                                    model_cls.objects.filter(pk=existing.pk).update(**update_fields)
                                    instance = model_cls.objects.get(pk=existing.pk)
                                else:
                                    last_errors.setdefault(model_label, f"IntegrityError: {exc}")
                                    next_pending.append(obj)
                                    continue
                            except Exception as exc2:
                                last_errors.setdefault(model_label, f"{type(exc2).__name__}: {exc2}")
                                next_pending.append(obj)
                                continue
                        else:
                            # If a unique constraint is hit, do not overwrite unknown existing rows.
                            # Defer so other deps can be created; if it still fails later, we'll error out.
                            last_errors.setdefault(model_label, f"IntegrityError: {exc}")
                            next_pending.append(obj)
                            continue
                    except Exception as exc:
                        last_errors.setdefault(model_label, f"{type(exc).__name__}: {exc}")
                        next_pending.append(obj)
                        continue

                    created_this_pass += 1

                    # Record PK mapping for any later FK remaps.
                    if _is_int_pk(old_pk):
                        pk_map.setdefault((app_label, model_name), {})[old_pk] = instance.pk

                    if m2m_values:
                        m2m_queue.append((instance, m2m_values))

                pending = next_pending
                log(
                    f"Import pass {pass_idx} created {created_this_pass} object(s); "
                    f"{len(pending)} still pending."
                )
                if created_this_pass == 0:
                    break

            if pending:
                # Provide a compact error with a few examples to help troubleshooting.
                examples = []
                for obj in pending[:10]:
                    examples.append(f"{obj.get('model')} pk={obj.get('pk')}")
                error_samples = []
                for k, v in list(last_errors.items())[:5]:
                    error_samples.append(f"{k}: {v}")
                raise CommandError(
                    "Import could not resolve dependencies for some objects. "
                    f"Remaining={len(pending)} Examples: {', '.join(examples)}"
                    + (f" Errors: {' | '.join(error_samples)}" if error_samples else "")
                )

            # Step 3: apply M2M relations (after all PKs are remapped).
            log(f"Applying M2M relationships for {len(m2m_queue)} object(s).")
            for instance, m2m_values in m2m_queue:
                model_cls = instance.__class__
                for m2m in model_cls._meta.many_to_many:
                    if m2m.name not in m2m_values:
                        continue
                    raw_list = m2m_values.get(m2m.name) or []
                    new_ids: list[int] = []
                    remote_model = m2m.remote_field.model
                    if remote_model == User:
                        for item in raw_list:
                            if _is_int_pk(item) and item in local_user_map:
                                new_ids.append(local_user_map[item])
                        try:
                            getattr(instance, m2m.name).set(new_ids)
                        except Exception:
                            pass
                        continue

                    r_key = _model_key(remote_model)
                    for item in raw_list:
                        if not _is_int_pk(item):
                            continue
                        if item in imported_pks.get(r_key, set()):
                            mapped = pk_map.get(r_key, {}).get(item)
                            if mapped is not None:
                                new_ids.append(mapped)
                        else:
                            new_ids.append(item)
                    try:
                        getattr(instance, m2m.name).set(new_ids)
                    except Exception:
                        pass

        self.stdout.write(self.style.SUCCESS("Expresslube import completed."))
